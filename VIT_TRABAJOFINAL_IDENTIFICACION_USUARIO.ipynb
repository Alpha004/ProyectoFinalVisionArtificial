{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ac√° comienza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import cv2, os, gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "root_path = \"F:/Maestria/Vision Artif/Finger-Vein-Recognition-2-master/Published_database_FV-USM_Dec2013/2nd_session/exc/\"\n",
    "\n",
    "folder_names = sorted(os.listdir(root_path))\n",
    "\n",
    "y_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIN\n"
     ]
    }
   ],
   "source": [
    "lista = []     \n",
    "x_list = []\n",
    "\n",
    "for folder in folder_names[2:]:\n",
    "    folder = folder\n",
    "    if int(len(lista))==4:\n",
    "        pre_lista_y = []\n",
    "        for fol in lista:\n",
    "            #print(lista)\n",
    "            dedo = glob(os.path.join(root_path, fol, '*'))            \n",
    "            imPaths = [path for path in dedo if '.jpg' in path]\n",
    "            #print(imPaths)            \n",
    "            for imPath in imPaths:\n",
    "                image = cv2.imread(imPath, cv2.IMREAD_GRAYSCALE)\n",
    "                image = cv2.resize(image, (84,84), cv2.INTER_AREA) #mandamos la imagen a cuadrado\n",
    "                x_list.append(image)                \n",
    "                post_fol = int(fol[4:-2])   # Hago un croping del name del folder para que no haya string y quede parte entera         \n",
    "                y_list.append(post_fol)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    if int(len(lista))== 4:\n",
    "        lista.clear()        \n",
    "    if int(len(lista))< 4:\n",
    "        lista.append(folder)\n",
    "    else:\n",
    "        pass    \n",
    "    continue\n",
    "print(\"FIN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{42, 43, 44, 45, 46, 47, 48, 49, 50, 51}\n"
     ]
    }
   ],
   "source": [
    "myset = set(y_list)\n",
    "print(myset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 84)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_list[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array(y_list)\n",
    "categories, inverse = np.unique(a, return_inverse=True)\n",
    "\n",
    "y_list_y = np.zeros((a.size, categories.size))\n",
    "y_list_y[np.arange(a.size), inverse] = 1   ## one hot encode\n",
    "\n",
    "x_list_x = np.asarray(x_list) ## pasar x's a un array de numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 84)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(216, 84, 84)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_list_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(216, 10)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_list_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "xuPMhXywkdm-"
   },
   "outputs": [],
   "source": [
    "#Import Package\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "#import tensorflow \n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input,Dense,Reshape,Flatten,Conv2D,Conv2DTranspose,LeakyReLU,ReLU,GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import BatchNormalization,Dropout,Embedding,Activation,Concatenate\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "#from tensorflow.keras.preprocessing.image import image\n",
    "from keras.utils.np_utils import to_categorical \n",
    "from tensorflow.keras import backend as K\n",
    "K.clear_session()\n",
    "#other library \n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "from PIL import Image\n",
    "import math\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import random\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "#installation\n",
    "#!pip install tensorflow-addons==0.8.3\n",
    "#import tensorflow_addons as tfa\n",
    "from mlxtend.plotting import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "w7yjJkN238Bi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last layer output shape:  (None, 2048)\n"
     ]
    }
   ],
   "source": [
    "#Model 1\n",
    "input_ = Input(shape=(84, 84,1))\n",
    "pre_trained_model = tf.keras.applications.inception_v3.InceptionV3(input_shape = (84, 84, 1), \n",
    "                                input_tensor = input_,\n",
    "                                include_top = False,\n",
    "                                weights = None \n",
    "                                )\n",
    "for layer in pre_trained_model.layers:\n",
    "    layer.trainable = True\n",
    "inception_layer = pre_trained_model.get_layer('mixed10')\n",
    "inception_layer = inception_layer.output\n",
    "inception_layer = layers.Flatten()(inception_layer)\n",
    "print('last layer output shape: ', inception_layer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "BAiwoTOJDZfp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last layer output shape:  (None, 18432)\n"
     ]
    }
   ],
   "source": [
    "#Model 2\n",
    "pre_trained_model_1 = tf.keras.applications.resnet50.ResNet50(input_shape = (84, 84, 1), \n",
    "                                input_tensor = input_,\n",
    "                                include_top = False, \n",
    "                                weights = None \n",
    "                                )\n",
    "for layer in pre_trained_model_1.layers:\n",
    "    layer.trainable = True\n",
    "resnet_layer = pre_trained_model_1.get_layer('conv5_block3_out')\n",
    "#pre_trained_model_1.summary()\n",
    "resnet_layer  = resnet_layer. output \n",
    "#x2 = layers.Conv2D(2048,(4,4),activation='relu')(x2)\n",
    "resnet_layer  = layers.Flatten()(resnet_layer )\n",
    "print('last layer output shape: ', resnet_layer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "D_6IxyuJ3rtP"
   },
   "outputs": [],
   "source": [
    "#Vision Transformer\n",
    "#Tuneble Prameters\n",
    "weight_decay = 0.0001\n",
    "num_epochs = 150\n",
    "num_classes= 10         # Tiene que la cantidad de clases de las personas\n",
    "image_size = 84   ### tiene que ser cuadrado\n",
    "patch_size = 6   \n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4          \n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "] \n",
    "transformer_layers = 8\n",
    "mlp_head_units = [2048, 1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "mR_CJF-i3r0Y"
   },
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "c_xAa4fs3sfb"
   },
   "outputs": [],
   "source": [
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "Ft_CXORZZI2Z"
   },
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "SD222oTCMIwS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last layer output shape:  (None, 1024)\n"
     ]
    }
   ],
   "source": [
    "patches = Patches(patch_size)(input_)\n",
    "encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "for _ in range(transformer_layers):\n",
    "    x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "    num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "    )(x1, x1)\n",
    "    x2 = layers.Add()([attention_output, encoded_patches])\n",
    "    x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "    x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "    encoded_patches = layers.Add()([x3, x2])\n",
    "representation = layers.LayerNormalization(name='last_layer',epsilon=1e-6)(encoded_patches)\n",
    "representation = layers.Flatten()(representation)\n",
    "representation = layers.Dropout(0.5)(representation) ## quiza cambair droputs\n",
    "representation= mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "print('last layer output shape: ', representation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "JL0T9-3d4P3U"
   },
   "outputs": [],
   "source": [
    "#Merge Model\n",
    "last_layer = layers.Concatenate()([inception_layer , resnet_layer, representation])\n",
    "x = layers.Dense(1024, activation='relu')(last_layer)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)                  \n",
    "x = layers.BatchNormalization(name='batch_1')(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "x = ReLU()(x)\n",
    "x = Dense(512)(x)\n",
    "x = layers.BatchNormalization(name='batch_2')(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "x = ReLU()(x)\n",
    "x = Dense(256)(x)\n",
    "x = BatchNormalization(name='batch_3')(x)\n",
    "x = ReLU()(x)\n",
    "x = layers.Dense(10, activation='softmax')(x) ## Densiddad de la ultiam layer tien q ser la cantidad de PERSONAS (clases)\n",
    "                                                #quizas camibar activacion, normalzaiciones \n",
    "                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "8kn4dY_YCgSa"
   },
   "outputs": [],
   "source": [
    "#Model Compile \n",
    "model = Model(inputs=input_,outputs= x)    \n",
    "model.compile(optimizer = Adam(learning_rate=0.001), ## Quiza cmabir optimizer y el learning rate\n",
    "              loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False), \n",
    "              metrics = ['accuracy','Precision', 'Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(216, 84, 84)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_list_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(216, 10)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_list_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_list_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "cviyk1V8Xvnh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "7/7 [==============================] - 37s 3s/step - loss: 2.4206 - accuracy: 0.1667 - precision: 0.4762 - recall: 0.0463\n",
      "Epoch 2/25\n",
      "7/7 [==============================] - 21s 3s/step - loss: 2.1305 - accuracy: 0.2824 - precision: 0.6667 - recall: 0.0833\n",
      "Epoch 3/25\n",
      "7/7 [==============================] - 21s 3s/step - loss: 1.6108 - accuracy: 0.4167 - precision: 0.8448 - recall: 0.2269\n",
      "Epoch 4/25\n",
      "7/7 [==============================] - 20s 3s/step - loss: 1.3522 - accuracy: 0.5324 - precision: 0.8495 - recall: 0.3657\n",
      "Epoch 5/25\n",
      "7/7 [==============================] - 21s 3s/step - loss: 1.1053 - accuracy: 0.6389 - precision: 0.8067 - recall: 0.4444\n",
      "Epoch 6/25\n",
      "7/7 [==============================] - 21s 3s/step - loss: 0.7839 - accuracy: 0.7269 - precision: 0.8621 - recall: 0.5787\n",
      "Epoch 7/25\n",
      "7/7 [==============================] - 20s 3s/step - loss: 0.4425 - accuracy: 0.8796 - precision: 0.9438 - recall: 0.7778\n",
      "Epoch 8/25\n",
      "7/7 [==============================] - 20s 3s/step - loss: 0.3187 - accuracy: 0.9120 - precision: 0.9543 - recall: 0.8704\n",
      "Epoch 9/25\n",
      "7/7 [==============================] - 20s 3s/step - loss: 0.2089 - accuracy: 0.9537 - precision: 0.9528 - recall: 0.9352\n",
      "Epoch 10/25\n",
      "7/7 [==============================] - 20s 3s/step - loss: 0.0753 - accuracy: 0.9907 - precision: 0.9907 - recall: 0.9861\n",
      "Epoch 11/25\n",
      "7/7 [==============================] - 20s 3s/step - loss: 0.0668 - accuracy: 0.9861 - precision: 0.9907 - recall: 0.9815\n",
      "Epoch 12/25\n",
      "7/7 [==============================] - 21s 3s/step - loss: 0.0635 - accuracy: 0.9907 - precision: 0.9907 - recall: 0.9861\n",
      "Epoch 13/25\n",
      "7/7 [==============================] - 21s 3s/step - loss: 0.0302 - accuracy: 0.9907 - precision: 0.9953 - recall: 0.9907\n",
      "Epoch 14/25\n",
      "7/7 [==============================] - 21s 3s/step - loss: 0.0167 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000\n",
      "Epoch 15/25\n",
      "7/7 [==============================] - 20s 3s/step - loss: 0.0267 - accuracy: 0.9954 - precision: 0.9954 - recall: 0.9954\n",
      "Epoch 16/25\n",
      "7/7 [==============================] - 20s 3s/step - loss: 0.0307 - accuracy: 0.9954 - precision: 0.9954 - recall: 0.9954\n",
      "Epoch 17/25\n",
      "7/7 [==============================] - 20s 3s/step - loss: 0.0755 - accuracy: 0.9769 - precision: 0.9769 - recall: 0.9769\n",
      "Epoch 18/25\n",
      "7/7 [==============================] - 20s 3s/step - loss: 0.0883 - accuracy: 0.9722 - precision: 0.9722 - recall: 0.9722\n",
      "Epoch 19/25\n",
      "7/7 [==============================] - 20s 3s/step - loss: 0.1897 - accuracy: 0.9583 - precision: 0.9579 - recall: 0.9491\n",
      "Epoch 20/25\n",
      "7/7 [==============================] - 20s 3s/step - loss: 0.1716 - accuracy: 0.9537 - precision: 0.9665 - recall: 0.9352\n",
      "Epoch 21/25\n",
      "7/7 [==============================] - 20s 3s/step - loss: 0.1339 - accuracy: 0.9769 - precision: 0.9813 - recall: 0.9722\n",
      "Epoch 22/25\n",
      "7/7 [==============================] - 20s 3s/step - loss: 0.1280 - accuracy: 0.9583 - precision: 0.9714 - recall: 0.9444\n",
      "Epoch 23/25\n",
      "7/7 [==============================] - 20s 3s/step - loss: 0.0729 - accuracy: 0.9722 - precision: 0.9767 - recall: 0.9722\n",
      "Epoch 24/25\n",
      "7/7 [==============================] - 20s 3s/step - loss: 0.0537 - accuracy: 0.9769 - precision: 0.9769 - recall: 0.9769\n",
      "Epoch 25/25\n",
      "7/7 [==============================] - 20s 3s/step - loss: 0.0272 - accuracy: 0.9907 - precision: 0.9907 - recall: 0.9907\n"
     ]
    }
   ],
   "source": [
    "#Model \n",
    "history = model.fit(\n",
    "            x = x_list_x,\n",
    "            y = y_list_y,\n",
    "            #,\n",
    "            epochs = 25,\n",
    "            verbose = 1,\n",
    "            #callbacks=[ ], #add here\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "ye4AFowt24JM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216/216 [==============================] - 15s 57ms/step\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(\n",
    "    x_list_x, batch_size=1, verbose='auto', steps=None, callbacks=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00811594, 0.0100297 , 0.00157174, 0.00929043, 0.00348545,\n",
       "       0.04053709, 0.47088155, 0.01255544, 0.4401481 , 0.00338458],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESULTADOS - IDENTIFICACION\n",
    "\n",
    "Como puede se observar en la imagen, se busca identificar el usuario al cual pertenece las imagenes, dandonos los porcentajes de los posibles candidatos, dandonos el usuario 48 como mayor porcentaje de probabilidad,¬†siendo¬†un¬†61%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m-6j6FTN24Sa"
   },
   "outputs": [],
   "source": [
    "#Terminate\n",
    "import os, signal\n",
    "os.kill(os.getpid(), signal.SIGKILL)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

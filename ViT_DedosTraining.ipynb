{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX8mhOLljYeM"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-_XNQDMF8IE"
      },
      "outputs": [],
      "source": [
        "!unzip /content/exc.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "widif6DGGmtN",
        "outputId": "1e76ce83-13f8-4fe9-f1e6-dce82a029312"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: tensorflow 2.10.0\n",
            "Uninstalling tensorflow-2.10.0:\n",
            "  Successfully uninstalled tensorflow-2.10.0\n",
            "Collecting tensorflow==2.15\n",
            "  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (24.1)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.15)\n",
            "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (1.64.1)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m99.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras<2.16,>=2.15.0 (from tensorflow==2.15)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (2.27.0)\n",
            "Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard<2.16,>=2.15->tensorflow==2.15)\n",
            "  Downloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (2.31.0)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.16,>=2.15->tensorflow==2.15)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard-data-server, protobuf, keras, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.10.0\n",
            "    Uninstalling tensorflow-estimator-2.10.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.10.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.6.1\n",
            "    Uninstalling tensorboard-data-server-0.6.1:\n",
            "      Successfully uninstalled tensorboard-data-server-0.6.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.10.0\n",
            "    Uninstalling keras-2.10.0:\n",
            "      Successfully uninstalled keras-2.10.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 0.4.6\n",
            "    Uninstalling google-auth-oauthlib-0.4.6:\n",
            "      Successfully uninstalled google-auth-oauthlib-0.4.6\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.10.1\n",
            "    Uninstalling tensorboard-2.10.1:\n",
            "      Successfully uninstalled tensorboard-2.10.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.25.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-auth-oauthlib-1.2.0 keras-2.15.0 protobuf-4.25.3 tensorboard-2.15.2 tensorboard-data-server-0.7.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "9fd114fac90f46f283c581883c66a4a6",
              "pip_warning": {
                "packages": [
                  "keras",
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip uninstall tensorflow -y\n",
        "!pip install  tensorflow==2.15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmMUeMfEFv5I",
        "outputId": "fa854776-7533-4863-d4b2-6ec4339b5bbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  1\n",
            "2.15.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1MF81o-EFv5I"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from glob import glob\n",
        "import cv2, os, gc\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "root_path = \"/content/exc/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j649l4jxFv5J",
        "outputId": "a89efa41-b974-4391-bd73-4cd0e289bc9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FIN\n"
          ]
        }
      ],
      "source": [
        "folder_names = sorted(os.listdir(root_path))\n",
        "\n",
        "y_list = []\n",
        "lista = []\n",
        "x_list = []\n",
        "\n",
        "for fol in folder_names[:]:\n",
        "    left = glob(os.path.join(root_path, fol, '*'))\n",
        "    imPaths = [path for path in left if '.jpg' in path]\n",
        "    for imPath in imPaths:\n",
        "        #image = cv2.imread(imPath, cv2.IMREAD_GRAYSCALE)\n",
        "        image = cv2.imread(imPath)\n",
        "        image = cv2.resize(image, (84,84), cv2.INTER_AREA) #reisezeamos al tamaño de entrada\n",
        "        x_list.append(image)\n",
        "        post_fol = str(fol[4:])# Hago un croping del name del folder para que no haya string y quede parte entera\n",
        "        s = post_fol.replace(\"_\", \"\")\n",
        "        s = int(s)\n",
        "        y_list.append(s)\n",
        "\n",
        "    continue\n",
        "print(\"FIN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "z5Fs6AzuFv5J"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "a = np.array(y_list)\n",
        "categories, inverse = np.unique(a, return_inverse=True)\n",
        "\n",
        "y_list_y = np.zeros((a.size, categories.size))\n",
        "y_list_y[np.arange(a.size), inverse] = 1   ## one hot encode\n",
        "\n",
        "x_list_x = np.asarray(x_list) ## pasar x's a un array de numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kz41YB-zFv5J",
        "outputId": "15190cd7-3840-4e4f-9cfe-8ee9f75ebf97"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(264, 84, 84, 3)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_list_x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRVJ3qAbFv5K",
        "outputId": "d2677dbb-482f-43db-f08b-de40b3db8a87"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.]])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_list_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4OWgdp4Fv5K",
        "outputId": "067a139c-aca0-486d-d957-0a196a26fd78"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(264, 44)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_list_y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPRVx1_NFv5K"
      },
      "source": [
        "### Acá comienza"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AZSXLWuFv5L"
      },
      "source": [
        "### Importante"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "xuPMhXywkdm-"
      },
      "outputs": [],
      "source": [
        "#Import Package\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "#import tensorflow\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Input,Dense,Reshape,Flatten,Conv2D,Conv2DTranspose,LeakyReLU,ReLU,GlobalAveragePooling1D\n",
        "from tensorflow.keras.layers import BatchNormalization,Dropout,Embedding,Activation,Concatenate\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "#from tensorflow.keras.preprocessing.image import image\n",
        "#from keras.utils.np_utils import to_categorical\n",
        "from tensorflow.keras import backend as K\n",
        "K.clear_session()\n",
        "#other library\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "from PIL import Image\n",
        "import math\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import random\n",
        "from tensorboard.plugins.hparams import api as hp\n",
        "#installation\n",
        "#!pip install tensorflow-addons==0.8.3\n",
        "#import tensorflow_addons as tfa\n",
        "from mlxtend.plotting import plot_confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0jiS26-DHu_",
        "outputId": "ee883b48-9d4d-4860-f43d-35e80e956d67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 264 images belonging to 44 classes.\n"
          ]
        }
      ],
      "source": [
        "#Data Preprocessing with augmentation\n",
        "\n",
        "TRAINING_DIR = \"/content/exc/\" #Training Set Directory\n",
        "training_datagen = ImageDataGenerator(rescale = 1./255,)\n",
        "\n",
        "train_generator = training_datagen.flow_from_directory(\n",
        "    TRAINING_DIR,\n",
        "    target_size=(84,84),\n",
        "    class_mode='categorical',\n",
        "    batch_size=16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7yjJkN238Bi",
        "outputId": "43a781d1-f35b-44cf-a94b-fe1764762f62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "last layer output shape:  (None, 2048)\n"
          ]
        }
      ],
      "source": [
        "#Model 1\n",
        "input_ = Input(shape=(84, 84,3))\n",
        "pre_trained_model = tf.keras.applications.inception_v3.InceptionV3(input_shape = (84, 84, 3),\n",
        "                                input_tensor = input_,\n",
        "                                include_top = False,\n",
        "                                weights = None\n",
        "                                )\n",
        "for layer in pre_trained_model.layers:\n",
        "    layer.trainable = True\n",
        "inception_layer = pre_trained_model.get_layer('mixed10')\n",
        "inception_layer = inception_layer.output\n",
        "inception_layer = layers.Flatten()(inception_layer)\n",
        "print('last layer output shape: ', inception_layer.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAiwoTOJDZfp",
        "outputId": "74fd2f78-517e-44ae-a308-fff7e6d855d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "last layer output shape:  (None, 18432)\n"
          ]
        }
      ],
      "source": [
        "#Model 2\n",
        "pre_trained_model_1 = tf.keras.applications.resnet50.ResNet50(input_shape = (84, 84, 3),\n",
        "                                input_tensor = input_,\n",
        "                                include_top = False,\n",
        "                                weights = None\n",
        "                                )\n",
        "for layer in pre_trained_model_1.layers:\n",
        "    layer.trainable = True\n",
        "resnet_layer = pre_trained_model_1.get_layer('conv5_block3_out')\n",
        "#pre_trained_model_1.summary()\n",
        "resnet_layer  = resnet_layer. output\n",
        "#x2 = layers.Conv2D(2048,(4,4),activation='relu')(x2)\n",
        "resnet_layer  = layers.Flatten()(resnet_layer )\n",
        "print('last layer output shape: ', resnet_layer.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "D_6IxyuJ3rtP"
      },
      "outputs": [],
      "source": [
        "#Vision Transformer\n",
        "#Tuneble Prameters\n",
        "weight_decay = 0.0001\n",
        "num_epochs = 150\n",
        "num_classes= 44         # Tiene que la cantidad de clases de las personas\n",
        "image_size = 84   ### tiene que ser cuadrado\n",
        "patch_size = 6\n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "projection_dim = 64\n",
        "num_heads = 4\n",
        "transformer_units = [\n",
        "    projection_dim * 2,\n",
        "    projection_dim,\n",
        "]\n",
        "transformer_layers = 8\n",
        "mlp_head_units = [2048, 1024]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "mR_CJF-i3r0Y"
      },
      "outputs": [],
      "source": [
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "c_xAa4fs3sfb"
      },
      "outputs": [],
      "source": [
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1.0, self.patch_size, self.patch_size, 1.0],\n",
        "            strides=[1.0, self.patch_size, self.patch_size, 1.0],\n",
        "            rates=[1.0, 1.0, 1.0, 1.0],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = tf.shape(patches)[-1]\n",
        "        #patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Ft_CXORZZI2Z"
      },
      "outputs": [],
      "source": [
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SD222oTCMIwS",
        "outputId": "b7121b43-36b6-4f51-bfab-50e9e548c88b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "last layer output shape:  (None, 1024)\n"
          ]
        }
      ],
      "source": [
        "patches = Patches(patch_size)(input_)\n",
        "encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "\n",
        "for _ in range(transformer_layers):\n",
        "    x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    attention_output = layers.MultiHeadAttention(\n",
        "    num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "    )(x1, x1)\n",
        "    x2 = layers.Add()([attention_output, encoded_patches])\n",
        "    x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "    x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "    encoded_patches = layers.Add()([x3, x2])\n",
        "representation = layers.LayerNormalization(name='last_layer',epsilon=1e-6)(encoded_patches)\n",
        "representation = layers.Flatten()(representation)\n",
        "representation = layers.Dropout(0.5)(representation) ## quiza cambair droputs\n",
        "representation= mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
        "print('last layer output shape: ', representation.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "JL0T9-3d4P3U"
      },
      "outputs": [],
      "source": [
        "#Merge Model\n",
        "last_layer = layers.Concatenate()([inception_layer , resnet_layer, representation])\n",
        "x = layers.Dense(1024, activation='relu')(last_layer)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "x = layers.BatchNormalization(name='batch_1')(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "x = ReLU()(x)\n",
        "x = Dense(512)(x)\n",
        "x = layers.BatchNormalization(name='batch_2')(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "x = ReLU()(x)\n",
        "x = Dense(256)(x)\n",
        "x = BatchNormalization(name='batch_3')(x)\n",
        "x = ReLU()(x)\n",
        "x = layers.Dense(44, activation='softmax')(x) ## Densiddad de la ultiam layer tien q ser la cantidad de PERSONAS (clases)\n",
        "                                                #quizas camibar activacion, normalzaiciones\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "8kn4dY_YCgSa"
      },
      "outputs": [],
      "source": [
        "#Model Compile\n",
        "model = Model(inputs=input_,outputs= x)\n",
        "model.compile(optimizer = Adam(learning_rate=0.001), ## Quiza cmabir optimizer y el learning rate\n",
        "              loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
        "              metrics = ['accuracy','Precision', 'Recall'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cviyk1V8Xvnh",
        "outputId": "db397cb7-8b9e-40f4-a0e1-1208c2db9b7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "17/17 [==============================] - 98s 214ms/step - loss: 3.9152 - accuracy: 0.0455 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
            "Epoch 2/30\n",
            "17/17 [==============================] - 3s 162ms/step - loss: 3.5433 - accuracy: 0.0909 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
            "Epoch 3/30\n",
            "17/17 [==============================] - 3s 162ms/step - loss: 3.1548 - accuracy: 0.1591 - precision: 0.2857 - recall: 0.0076\n",
            "Epoch 4/30\n",
            "17/17 [==============================] - 3s 161ms/step - loss: 2.7807 - accuracy: 0.1856 - precision: 0.1818 - recall: 0.0076\n",
            "Epoch 5/30\n",
            "17/17 [==============================] - 3s 187ms/step - loss: 2.5087 - accuracy: 0.2727 - precision: 0.6471 - recall: 0.0417\n",
            "Epoch 6/30\n",
            "17/17 [==============================] - 3s 163ms/step - loss: 2.2530 - accuracy: 0.3068 - precision: 0.5758 - recall: 0.0720\n",
            "Epoch 7/30\n",
            "17/17 [==============================] - 3s 163ms/step - loss: 2.1984 - accuracy: 0.3939 - precision: 0.7353 - recall: 0.0947\n",
            "Epoch 8/30\n",
            "17/17 [==============================] - 4s 217ms/step - loss: 1.9734 - accuracy: 0.4432 - precision: 0.7609 - recall: 0.1326\n",
            "Epoch 9/30\n",
            "17/17 [==============================] - 3s 162ms/step - loss: 1.6268 - accuracy: 0.5379 - precision: 0.8281 - recall: 0.2008\n",
            "Epoch 10/30\n",
            "17/17 [==============================] - 3s 165ms/step - loss: 1.3980 - accuracy: 0.6402 - precision: 0.8415 - recall: 0.2614\n",
            "Epoch 11/30\n",
            "17/17 [==============================] - 3s 163ms/step - loss: 1.1744 - accuracy: 0.7008 - precision: 0.8846 - recall: 0.3485\n",
            "Epoch 12/30\n",
            "17/17 [==============================] - 3s 196ms/step - loss: 1.1653 - accuracy: 0.6894 - precision: 0.8300 - recall: 0.3144\n",
            "Epoch 13/30\n",
            "17/17 [==============================] - 3s 167ms/step - loss: 0.9574 - accuracy: 0.7424 - precision: 0.8963 - recall: 0.4583\n",
            "Epoch 14/30\n",
            "17/17 [==============================] - 3s 164ms/step - loss: 1.3361 - accuracy: 0.6098 - precision: 0.8041 - recall: 0.2955\n",
            "Epoch 15/30\n",
            "17/17 [==============================] - 3s 163ms/step - loss: 0.9648 - accuracy: 0.7424 - precision: 0.8903 - recall: 0.5227\n",
            "Epoch 16/30\n",
            "17/17 [==============================] - 3s 183ms/step - loss: 0.7919 - accuracy: 0.7803 - precision: 0.8933 - recall: 0.6023\n",
            "Epoch 17/30\n",
            "17/17 [==============================] - 3s 195ms/step - loss: 0.6337 - accuracy: 0.8371 - precision: 0.9541 - recall: 0.7083\n",
            "Epoch 18/30\n",
            "17/17 [==============================] - 3s 165ms/step - loss: 0.5395 - accuracy: 0.8636 - precision: 0.9296 - recall: 0.7500\n",
            "Epoch 19/30\n",
            "17/17 [==============================] - 3s 163ms/step - loss: 0.4183 - accuracy: 0.9053 - precision: 0.9462 - recall: 0.7992\n",
            "Epoch 20/30\n",
            "17/17 [==============================] - 3s 164ms/step - loss: 0.3307 - accuracy: 0.9129 - precision: 0.9623 - recall: 0.8712\n",
            "Epoch 21/30\n",
            "17/17 [==============================] - 4s 212ms/step - loss: 0.2834 - accuracy: 0.9470 - precision: 0.9793 - recall: 0.8939\n",
            "Epoch 22/30\n",
            "17/17 [==============================] - 3s 162ms/step - loss: 0.3328 - accuracy: 0.9318 - precision: 0.9614 - recall: 0.8485\n",
            "Epoch 23/30\n",
            "17/17 [==============================] - 3s 166ms/step - loss: 0.2574 - accuracy: 0.9508 - precision: 0.9755 - recall: 0.9053\n",
            "Epoch 24/30\n",
            "17/17 [==============================] - 3s 203ms/step - loss: 0.2022 - accuracy: 0.9659 - precision: 0.9881 - recall: 0.9432\n",
            "Epoch 25/30\n",
            "17/17 [==============================] - 3s 165ms/step - loss: 0.1632 - accuracy: 0.9621 - precision: 0.9841 - recall: 0.9394\n",
            "Epoch 26/30\n",
            "17/17 [==============================] - 3s 162ms/step - loss: 0.1180 - accuracy: 0.9697 - precision: 0.9697 - recall: 0.9697\n",
            "Epoch 27/30\n",
            "17/17 [==============================] - 3s 205ms/step - loss: 0.1715 - accuracy: 0.9735 - precision: 0.9842 - recall: 0.9432\n",
            "Epoch 28/30\n",
            "17/17 [==============================] - 3s 161ms/step - loss: 0.1087 - accuracy: 0.9886 - precision: 0.9923 - recall: 0.9735\n",
            "Epoch 29/30\n",
            "17/17 [==============================] - 3s 162ms/step - loss: 0.0906 - accuracy: 0.9924 - precision: 0.9962 - recall: 0.9886\n",
            "Epoch 30/30\n",
            "17/17 [==============================] - 3s 162ms/step - loss: 0.1438 - accuracy: 0.9811 - precision: 0.9847 - recall: 0.9735\n"
          ]
        }
      ],
      "source": [
        "#Model\n",
        "history = model.fit(\n",
        "                train_generator,\n",
        "#             x = x_list_x,\n",
        "#             y = y_list_y,\n",
        "            epochs = 30,\n",
        "            verbose = 1,\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RESOLUCION\n",
        "\n",
        "Se entreno el modelo con todos los dedos, obteniendo como parte de validacion del entrenamiento altos niveles de accuracy y precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vk8a8B30Fv5N",
        "outputId": "20843d97-c63c-48f0-df25-2247ece7b2bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17/17 [==============================] - 6s 59ms/step\n"
          ]
        }
      ],
      "source": [
        "prediction = model.predict(\n",
        "    train_generator, verbose='auto', steps=None, callbacks=None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLNg3qDuFv5N",
        "outputId": "0b0684fa-315c-4d78-a1a6-dda024bdbaaa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1.8838522e-04, 3.6817003e-04, 7.8980130e-04, ..., 3.8209397e-04,\n",
              "        7.9809601e-05, 4.6917489e-03],\n",
              "       [4.2757654e-04, 3.6888971e-04, 1.3049397e-03, ..., 4.3699294e-04,\n",
              "        9.4055176e-05, 7.3122135e-03],\n",
              "       [9.4765233e-04, 1.9827638e-04, 1.4930047e-03, ..., 2.6103374e-04,\n",
              "        4.8726488e-05, 1.9566666e-03],\n",
              "       ...,\n",
              "       [3.5708238e-04, 3.2209119e-04, 1.4421352e-03, ..., 2.7774327e-04,\n",
              "        6.4025066e-05, 4.9938834e-03],\n",
              "       [1.9607815e-04, 3.5304183e-04, 6.3665729e-04, ..., 2.7991409e-04,\n",
              "        5.7735673e-05, 4.4927429e-03],\n",
              "       [1.9809089e-04, 3.5531036e-04, 6.5199059e-04, ..., 2.9911712e-04,\n",
              "        6.2356026e-05, 4.5604524e-03]], dtype=float32)"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "-nPCjZKpOG67"
      },
      "outputs": [],
      "source": [
        "numbers = []\n",
        "for pred in prediction:\n",
        "  formatted_numbers = [format(num, '.6f') for num in pred]\n",
        "  numbers.append(formatted_numbers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LKEJ1xfOG4t",
        "outputId": "6cda3e35-bf66-45ad-b669-c5ab409982ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.413828\n"
          ]
        }
      ],
      "source": [
        "print(max(numbers[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0COqCL3jOWWj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-6j6FTN24Sa"
      },
      "outputs": [],
      "source": [
        "#Terminate\n",
        "import os, signal\n",
        "os.kill(os.getpid(), signal.SIGKILL)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
